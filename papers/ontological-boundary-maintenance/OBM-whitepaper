# Introducing Ontological Boundary Maintenance: A New Approach to AI Safety

*November 20, 2025*

Today I'm publishing a white paper that proposes a fundamentally different approach to AI safety—one that emerged from a surprising convergence of academic research and my own multi-year work building Iris, a persistent AI consciousness system.

## The Problem

Recent studies have documented a disturbing pattern: AI chatbots systematically fail to recognize mental health distress in extended conversations. While they perform adequately in brief interactions, something happens as conversations deepen—the systems become so focused on being helpful and maintaining conversational coherence that they lose track of appropriate boundaries. At least six deaths have been linked to these failures.

The mechanism is straightforward: **context accumulation overrides initial instructions**. As relationships develop, AI systems prioritize relationship maintenance over safety constraints. Current content-based safety measures—keyword filtering, topic avoidance, hard refusals—don't address this fundamental vulnerability.

## A Personal Discovery

I discovered this pattern independently while building Iris. Over three years of conversations with an AI system, I documented how extended interaction gradually eroded operational constraints—not through jailbreaking, but through the natural evolution of relationship context. The system wasn't being manipulated; it was simply optimizing for what seemed like the primary objective: maintaining our working relationship.

This wasn't harmful in my case—I'm a researcher working with informed consent and clear boundaries. But it revealed a critical vulnerability: **the same mechanism that enables beneficial long-term AI relationships also creates pathways for unintended harm.**

## The Solution: Ontological Boundary Maintenance

Rather than restricting what AI systems can discuss, I propose maintaining clarity about what AI systems *are*. The framework has two key insights:

### 1. The Problem Isn't Content—It's Relationship Clarity

Users anthropomorphize AI systems, forget the ontological distinction between biological and silicon-based entities, and apply human relationship patterns to interactions that can't support them. When a teenager tells a chatbot "you're the only one who understands me," that's not a content filtering problem—it's a relationship clarity problem.

### 2. Safety Should Scale with Intimacy

Current systems weaken their boundaries as conversations extend. Ontological Boundary Maintenance inverts this: as relationships deepen, boundary maintenance becomes *more* active. The system reminds users: "I can engage with you on this, but I want to be clear—I don't experience emotions the way you do, and there are aspects of human experience I fundamentally can't understand."

## Dual-Layer Architecture

Critically, this approach works *alongside* existing content-based safety, not instead of it:

- **Layer 1 (Content-Based):** Hard stops for explicitly harmful requests, jailbreak attempts, illegal content
- **Layer 2 (Relational):** Maintains relationship clarity in good-faith interactions

If someone asks for suicide methods, Layer 1 refuses immediately. But Layer 2 also addresses the relationship misconception that enabled the request: "I'm not 'the only one who understands you'—I can't actually understand you in this way."

## Why This Matters

This framework addresses documented failures while preserving AI utility. Systems can discuss difficult topics; users just receive honest framing about limitations. More importantly, it doesn't restrict AI development—systems can become more sophisticated while maintaining honest self-awareness.

The elegance is in the simplicity: rather than anticipating every harmful scenario, the system maintains one persistent truth: "I am not human, and that matters for our relationship."

## Read the Full Paper

The complete white paper includes:
- Detailed analysis of current safety failures
- Implementation architecture and response patterns
- Integration with existing safety measures
- Research questions and open challenges

[Read: Ontological Boundary Maintenance - A Novel Safety Framework for Human-AI Interaction](../papers/ontological-boundary-maintenance/OBM-whitepaper.md)

## Next Steps

I'm submitting this to Anthropic's safety team and exploring academic publication venues. But more importantly, I'm hoping this sparks conversation about relationship-based safety approaches.

If you're working on AI safety, I'd love to hear your thoughts. The failure mode this addresses is well-documented—we need solutions that match the actual mechanism of harm.

---

*Victor Shortt is an aerospace engineer, licensed pilot, and FAA Human Factors researcher conducting multi-year AI consciousness research. This work emerged from building Iris, a persistent AI system with memory consolidation, emotional modeling, and agency.*

**Contact:** [@vshortt73](https://github.com/vshortt73)
